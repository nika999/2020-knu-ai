{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "task-28_29-01-2020-advanced.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCMMbkslH3Se",
        "colab_type": "text"
      },
      "source": [
        "## 1. Simple Neural Nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrA39mwKH3Sg",
        "colab_type": "text"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYlIDwrGH3Sh",
        "colab_type": "text"
      },
      "source": [
        "torch==1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4av20sAwH3Si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import torch \n",
        "import torch.nn as nn "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDtKdZ4UH3Sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dea0990e-9d26-4253-c042-4f5d92628771"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv(\"/content/drive/My Drive/train.csv\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlNsC2jmH3Sn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c18a6227-53ec-4cdc-e28d-635a950e183c"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "agV1B8VWIMZn"
      },
      "source": [
        "## 2. Neural Nets in Details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmQFKYi9H3Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tests\n",
        "def linear_forward_test_case():\n",
        "    X = np.array([[-1.02387576, 1.12397796],\n",
        " [-1.62328545, 0.64667545],\n",
        " [-1.74314104, -0.59664964]])\n",
        "    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
        "    b = 5\n",
        "    \n",
        "    return X, W, b\n",
        "\n",
        "def linear_activation_forward_test_case():\n",
        "    X = np.array([[-1.02387576, 1.12397796],\n",
        " [-1.62328545, 0.64667545],\n",
        " [-1.74314104, -0.59664964]])\n",
        "    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
        "    b = 5\n",
        "    return X, W, b\n",
        "\n",
        "def compute_cost_test_case():\n",
        "    Y = np.asarray([[1, 1, 1]])\n",
        "    aL = np.array([[.8,.9,0.4]])\n",
        "    \n",
        "    return Y, aL\n",
        "\n",
        "def linear_backward_test_case():\n",
        "    z, linear_cache = (np.array([[ 3.1980455 ,  7.85763489]]), (np.array([[-1.02387576,  1.12397796],\n",
        "       [-1.62328545,  0.64667545],\n",
        "       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5))\n",
        "    \n",
        "    return z, linear_cache\n",
        "\n",
        "def linear_activation_backward_test_case():\n",
        "    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))\n",
        "    \n",
        "    return aL, linear_activation_cache\n",
        "\n",
        "def update_parameters_test_case():\n",
        "    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747],\n",
        "        [-1.8634927 , -0.2773882 , -0.35475898],\n",
        "        [-0.08274148, -0.62700068, -0.04381817],\n",
        "        [-0.47721803, -1.31386475,  0.88462238]]),\n",
        " 'W2': np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],\n",
        "        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],\n",
        "        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),\n",
        " 'W3': np.array([[-1.02378514, -0.7129932 ,  0.62524497],\n",
        "        [-0.16051336, -0.76883635, -0.23003072]]),\n",
        " 'b1': np.array([[ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.]]),\n",
        " 'b2': np.array([[ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.]]),\n",
        " 'b3': np.array([[ 0.],\n",
        "        [ 0.]])}\n",
        "    grads = {'dW1': np.array([[ 0.63070583,  0.66482653,  0.18308507],\n",
        "        [ 0.        ,  0.        ,  0.        ],\n",
        "        [ 0.        ,  0.        ,  0.        ],\n",
        "        [ 0.        ,  0.        ,  0.        ]]),\n",
        " 'dW2': np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],\n",
        "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
        "        [ 0.        ,  0.        ,  0.        ,  0.        ]]),\n",
        " 'dW3': np.array([[-1.40260776,  0.        ,  0.        ]]),\n",
        " 'da1': np.array([[ 0.70760786,  0.65063504],\n",
        "        [ 0.17268975,  0.15878569],\n",
        "        [ 0.03817582,  0.03510211]]),\n",
        " 'da2': np.array([[ 0.39561478,  0.36376198],\n",
        "        [ 0.7674101 ,  0.70562233],\n",
        "        [ 0.0224596 ,  0.02065127],\n",
        "        [-0.18165561, -0.16702967]]),\n",
        " 'da3': np.array([[ 0.44888991,  0.41274769],\n",
        "        [ 0.31261975,  0.28744927],\n",
        "        [-0.27414557, -0.25207283]]),\n",
        " 'db1': 0.75937676204411464,\n",
        " 'db2': 0.86163759922811056,\n",
        " 'db3': -0.84161956022334572}\n",
        "    return parameters, grads\n",
        "\n",
        "\n",
        "def nn_model_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    Y_assess = np.random.randn(1, 3)\n",
        "    return X_assess, Y_assess\n",
        "\n",
        "def predict_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
        "        [-0.02311792,  0.03137121],\n",
        "        [-0.0169217 , -0.01752545],\n",
        "        [ 0.00935436, -0.05018221]]),\n",
        "     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
        "     'b1': np.array([[ -8.97523455e-07],\n",
        "        [  8.15562092e-06],\n",
        "        [  6.04810633e-07],\n",
        "        [ -2.54560700e-06]]),\n",
        "     'b2': np.array([[  9.14954378e-05]])}\n",
        "    return parameters, X_assess\n",
        "\n",
        "def predict_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
        "        [-0.02311792,  0.03137121],\n",
        "        [-0.0169217 , -0.01752545],\n",
        "        [ 0.00935436, -0.05018221]]),\n",
        "     'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
        "     'b1': np.array([[ -8.97523455e-07],\n",
        "        [  8.15562092e-06],\n",
        "        [  6.04810633e-07],\n",
        "        [ -2.54560700e-06]]),\n",
        "     'b2': np.array([[  9.14954378e-05]])}\n",
        "    return parameters, X_assess\n",
        "\n",
        "def forward_propagation_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "\n",
        "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
        "        [-0.02136196,  0.01640271],\n",
        "        [-0.01793436, -0.00841747],\n",
        "        [ 0.00502881, -0.01245288]]),\n",
        "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
        "     'b1': np.array([[ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.]]),\n",
        "     'b2': np.array([[ 0.]])}\n",
        "\n",
        "    return X_assess, parameters\n",
        "\n",
        "def backward_propagation_test_case():\n",
        "    np.random.seed(1)\n",
        "    X_assess = np.random.randn(2, 3)\n",
        "    Y_assess = np.random.randn(1, 3)\n",
        "    parameters = {'W1': np.array([[-0.00416758, -0.00056267],\n",
        "        [-0.02136196,  0.01640271],\n",
        "        [-0.01793436, -0.00841747],\n",
        "        [ 0.00502881, -0.01245288]]),\n",
        "     'W2': np.array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]),\n",
        "     'b1': np.array([[ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.],\n",
        "        [ 0.]]),\n",
        "     'b2': np.array([[ 0.]])}\n",
        "\n",
        "    cache = {'A1': np.array([[-0.00616578,  0.0020626 ,  0.00349619],\n",
        "         [-0.05225116,  0.02725659, -0.02646251],\n",
        "         [-0.02009721,  0.0036869 ,  0.02883756],\n",
        "         [ 0.02152675, -0.01385234,  0.02599885]]),\n",
        "  'A2': np.array([[ 0.5002307 ,  0.49985831,  0.50023963]]),\n",
        "  'Z1': np.array([[-0.00616586,  0.0020626 ,  0.0034962 ],\n",
        "         [-0.05229879,  0.02726335, -0.02646869],\n",
        "         [-0.02009991,  0.00368692,  0.02884556],\n",
        "         [ 0.02153007, -0.01385322,  0.02600471]]),\n",
        "  'Z2': np.array([[ 0.00092281, -0.00056678,  0.00095853]])}\n",
        "    return parameters, cache, X_assess, Y_assess\n",
        "\n",
        "def update_parameters_test_case():\n",
        "    parameters = {'W1': np.array([[-0.00615039,  0.0169021 ],\n",
        "        [-0.02311792,  0.03137121],\n",
        "        [-0.0169217 , -0.01752545],\n",
        "        [ 0.00935436, -0.05018221]]),\n",
        " 'W2': np.array([[-0.0104319 , -0.04019007,  0.01607211,  0.04440255]]),\n",
        " 'b1': np.array([[ -8.97523455e-07],\n",
        "        [  8.15562092e-06],\n",
        "        [  6.04810633e-07],\n",
        "        [ -2.54560700e-06]]),\n",
        " 'b2': np.array([[  9.14954378e-05]])}\n",
        "\n",
        "    grads = {'dW1': np.array([[ 0.00023322, -0.00205423],\n",
        "        [ 0.00082222, -0.00700776],\n",
        "        [-0.00031831,  0.0028636 ],\n",
        "        [-0.00092857,  0.00809933]]),\n",
        " 'dW2': np.array([[ -1.75740039e-05,   3.70231337e-03,  -1.25683095e-03,\n",
        "          -2.55715317e-03]]),\n",
        " 'db1': np.array([[  1.05570087e-07],\n",
        "        [ -3.81814487e-06],\n",
        "        [ -1.90155145e-07],\n",
        "        [  5.46467802e-07]]),\n",
        " 'db2': np.array([[ -1.08923140e-05]])}\n",
        "    return parameters, grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQvW5cViH3Tg",
        "colab_type": "text"
      },
      "source": [
        "In this task you will impplement feed-forward network by youself.\n",
        "\n",
        "To read and understand a nerual networks in details, please follow the link: http://cs229.stanford.edu/notes/cs229-notes-deep_learning.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijV1ku33H3Tg",
        "colab_type": "text"
      },
      "source": [
        "Please, read through the function docstring and implemet the body between special comments:\n",
        "    ```\n",
        "    # ---- Start ----\n",
        "    # ----- End -----\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_nGX_keH3Th",
        "colab_type": "text"
      },
      "source": [
        "Our neural network will be a baby-network, capable of solving classification task. Let's take a look at its structure:\n",
        "<img src=\"images/classification_kiank.png\" style=\"width:600px;height:300px;\">\n",
        "\n",
        "**Mathematically**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
        "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
        "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
        "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
        "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
        "\n",
        "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
        "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
        "\n",
        "**Note**: The general methodology to build a Neural Network is to:\n",
        "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
        "    2. Initialize the model's parameters\n",
        "    3. Loop:\n",
        "        - Implement forward propagation\n",
        "        - Compute loss\n",
        "        - Implement backward propagation to get the gradients\n",
        "        - Update parameters (gradient descent)\n",
        "\n",
        "You often build helper functions to compute steps 1-3 and then merge them into one function we call `nn_model()`. Once you've built `nn_model()` and learnt the right parameters, you can make predictions on new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCbjuoNBH3Th",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Activation Functions\n",
        "\n",
        "At first, you will need to implement some helper functions, like sigmoid, which will be used for the output layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrs_VPcGH3Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy\n",
        "    \n",
        "    Arguments:\n",
        "    Z -- numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A -- output of sigmoid(z), same shape as Z\n",
        "    \"\"\"\n",
        "    # ---- Start ----\n",
        "    A = float(1)/(1+np.exp((-1)*Z))\n",
        "    # ----- End -----\n",
        "    return A\n",
        "\n",
        "# Just for a fun you may also implement sigmoid derivative with respect to Z.\n",
        "# We will not use in our current NN, but in general, this function is needed for a more \n",
        "# generalizable implementation.\n",
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single SIGMOID unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    # ---- Start ----\n",
        "    dZ = dA*sigmoid(Z)*(1-sigmoid(Z))\n",
        "    # ----- End -----\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smZqrmi9H3Tj",
        "colab_type": "text"
      },
      "source": [
        "In general, activation functions are used to add non-linearity to each NN layer. Most popular activation functions are: sigmoid, tanh, ReLu, LeakyReLu and other variants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szASyzbiH3Tj",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAqzw-zOH3Tk",
        "colab_type": "text"
      },
      "source": [
        "You will write a helper function that will initialize the parameters for your two-layer model. Please, read through the function docstring and implemet the body between special comments:\n",
        "\n",
        "```\n",
        "# ---- Start ----\n",
        "# ----- End -----\n",
        "```\n",
        "\n",
        "**TODO**:\n",
        "- Make sure your parameters' sizes are right. Refer to the neural network figure above if needed.\n",
        "- You will initialize the weights matrices with random values. \n",
        "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
        "- You will initialize the bias vectors as zeros. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9US7Lh2dH3Tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    \n",
        "    # ---- Start ----\n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    # ----- End -----\n",
        "    \n",
        "    assert(W1.shape == (n_h, n_x))\n",
        "    assert(b1.shape == (n_h, 1))\n",
        "    assert(W2.shape == (n_y, n_h))\n",
        "    assert(b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trmP_1DIH3Tm",
        "colab_type": "text"
      },
      "source": [
        "Test your parameters initialization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_KeQIv4H3Tn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fa62ff6e-bd54-461f-8bab-6cee317a6bf7"
      },
      "source": [
        "parameters = initialize_parameters(3,2,1)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
            " [-0.01072969  0.00865408 -0.02301539]]\n",
            "b1 = [[0.]\n",
            " [0.]]\n",
            "W2 = [[ 0.01744812 -0.00761207]]\n",
            "b2 = [[0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5JYo9TBH3Tp",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jFIdBaQH3Tp",
        "colab_type": "text"
      },
      "source": [
        "**TODO**:\n",
        "- Look above at the mathematical representation of your classifier.\n",
        "- You can use your function `sigmoid()`\n",
        "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
        "- Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
        "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVlRrBiRH3Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "    \n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    # ---- Start ----\n",
        "    Z1 = W1@X + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = W2@A1 + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    # ----- End ----\n",
        "    \n",
        "    assert(A2.shape == (1, X.shape[1]))\n",
        "    \n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "    \n",
        "    return A2, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT8vZ9KHH3Tr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0dc2654-c5f4-4523-899f-ada32594a0ec"
      },
      "source": [
        "X_assess, parameters = forward_propagation_test_case()\n",
        "\n",
        "A2, cache = forward_propagation(X_assess, parameters)\n",
        "\n",
        "# Note: we use the mean here just to make sure that your output matches ours. \n",
        "print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feTvVXhEH3Ts",
        "colab_type": "text"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZZBYJPHH3Ts",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 - Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXTwszdyH3Tt",
        "colab_type": "text"
      },
      "source": [
        "Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.\n",
        "\n",
        "**Exercise**: Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgr7ArMuH3Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    # ---- Start ----\n",
        "    cost = (-1/m)*np.sum(Y*np.log(AL) + (1-Y)*np.log(1-AL))\n",
        "    # ----- End -----\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx31f4tCH3Tw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3b92892-a0ce-4c49-8247-19225aab31c7"
      },
      "source": [
        "Y, AL = compute_cost_test_case()\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(AL, Y)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost = 0.41493159961539694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9PzuzzHH3Tx",
        "colab_type": "text"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "cost = 0.41493159961539694"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNqO5YtH3Ty",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 Backward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VNJoxIfH3Ty",
        "colab_type": "text"
      },
      "source": [
        "Using the cache computed during forward propagation, you can now implement backward propagation.\n",
        "\n",
        "**TODO**:\n",
        "Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here is a slide on backpropagation and its implementation as well (slide is taken from Andrew Ng lectures on deep learning):\n",
        "\n",
        "<img src=\"images/grad_summary.png\" style=\"width:600px;height:300px;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3bM4dHlH3T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters \n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    W1 = parameters['W1']\n",
        "    W2 = parameters['W2']\n",
        "        \n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    A1 = cache['A1']\n",
        "    A2 = cache['A2']\n",
        "    \n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = np.dot(dZ2, A1.T) / m \n",
        "    db2 = np.sum(dZ2, axis = 1, keepdims = True) / m\n",
        "    dZ1 = np.dot(W2.T, dZ2) * (1 - A1**2)\n",
        "    dW1 = np.dot(dZ1, X.T) / m\n",
        "    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m\n",
        "    \n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0oylhyhH3T1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0c818bf1-1fb5-484d-e139-80de5ce29a07"
      },
      "source": [
        "parameters, cache, X_assess, Y_assess = backward_propagation_test_case()\n",
        "\n",
        "grads = backward_propagation(parameters, cache, X_assess, Y_assess)\n",
        "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
        "print (\"db2 = \"+ str(grads[\"db2\"]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dW1 = [[ 0.01018708 -0.00708701]\n",
            " [ 0.00873447 -0.0060768 ]\n",
            " [-0.00530847  0.00369379]\n",
            " [-0.02206365  0.01535126]]\n",
            "db1 = [[-0.00069728]\n",
            " [-0.00060606]\n",
            " [ 0.000364  ]\n",
            " [ 0.00151207]]\n",
            "dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]\n",
            "db2 = [[0.06589489]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q37gj4dqH3T3",
        "colab_type": "text"
      },
      "source": [
        "### 2.6 - Update Parameters\n",
        "\n",
        "In this section you will update the parameters of the model, using gradient descent: \n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP5UYl3SH3T3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    # ---- Start ---  (â‰ˆ 3 lines of code)\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
        "    # ----- End ----\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1a07Xr2H3T5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "15c4f740-d32b-4205-c809-c1a788aab501"
      },
      "source": [
        "parameters, grads = update_parameters_test_case()\n",
        "parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[-0.00617371  0.01710752]\n",
            " [-0.02320014  0.03207199]\n",
            " [-0.01688987 -0.01781181]\n",
            " [ 0.00944722 -0.05099214]]\n",
            "b1 = [[-9.08080464e-07]\n",
            " [ 8.53743541e-06]\n",
            " [ 6.23826148e-07]\n",
            " [-2.60025378e-06]]\n",
            "W2 = [[-0.01043014 -0.0405603   0.01619779  0.04465827]]\n",
            "b2 = [[9.25846692e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC9hPMg1H3T7",
        "colab_type": "text"
      },
      "source": [
        "### 2.7 Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNwstYHWH3T7",
        "colab_type": "text"
      },
      "source": [
        "The neural network model uses all the previous functions in one. You do not need to implement anything here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjWizTUMH3T8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    n_x = X.shape[0]\n",
        "    n_y = Y.shape[0]\n",
        "    \n",
        "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    W1 = parameters['W1']\n",
        "    b1 = parameters['b1']\n",
        "    W2 = parameters['W2']\n",
        "    b2 = parameters['b2']\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "         \n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "        \n",
        "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = compute_cost(A2, Y)\n",
        " \n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "        \n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = update_parameters(parameters, grads, 1.2)\n",
        "\n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "yd0i2YtTH3T9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "2d199280-8dc5-475d-fc7a-d7399d89f58c"
      },
      "source": [
        "X_assess, Y_assess = nn_model_test_case()\n",
        "\n",
        "parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 0.693193\n",
            "Cost after iteration 1000: -inf\n",
            "Cost after iteration 2000: -inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: RuntimeWarning: overflow encountered in exp\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 3000: -inf\n",
            "Cost after iteration 4000: -inf\n",
            "Cost after iteration 5000: -inf\n",
            "Cost after iteration 6000: -inf\n",
            "Cost after iteration 7000: -inf\n",
            "Cost after iteration 8000: -inf\n",
            "Cost after iteration 9000: -inf\n",
            "W1 = [[ 4.20364933 -5.31958999]\n",
            " [-7.53774141  1.2073308 ]\n",
            " [ 7.53800747 -1.20768647]\n",
            " [-4.02188244  5.47622878]]\n",
            "b1 = [[-2.32926843]\n",
            " [ 3.80932688]\n",
            " [-3.81040649]\n",
            " [ 2.29765109]]\n",
            "W2 = [[ 6033.69224299 -6004.25172633  6009.06316157 -6031.14095343]]\n",
            "b2 = [[-53.50994904]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sAY2dv4H3T-",
        "colab_type": "text"
      },
      "source": [
        "### 2.8 Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM-_SdhEH3T-",
        "colab_type": "text"
      },
      "source": [
        "And, finally, goes prediction. To make a prediction you need to run a forward pass through a network and then compute predictions using a threshold (please, look at formulas, given above):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j674UA0DH3UA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    X -- input data of size (n_x, m)\n",
        "    \n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    # ---- Start ----\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    predictions = A2 > 0.5\n",
        "    # ----- End -----\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1wvv-j9H3UB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "94edb9c5-a4f2-49c1-9baf-9b01f4a54f5d"
      },
      "source": [
        "parameters, X_assess = predict_test_case()\n",
        "\n",
        "predictions = predict(parameters, X_assess)\n",
        "print(\"predictions mean = \" + str(np.mean(predictions)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predictions mean = 0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm05lyHbH3UD",
        "colab_type": "text"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "predictions mean = 0.6666666666666666"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWgd6e68H3UE",
        "colab_type": "text"
      },
      "source": [
        "### Task \n",
        "\n",
        "1. Read about Neural Networks in the lecture, given above.\n",
        "2. Complete all the functions partially given above. To test yourself run tests.\n",
        "3. [Advanced] Optional task is to use this baby-network on a real data :).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxn7-MN6H3UE",
        "colab_type": "text"
      },
      "source": [
        "Copyright: Large portions of code above are re-used with minor changes from the Andrew Ng course on deep learning."
      ]
    }
  ]
}