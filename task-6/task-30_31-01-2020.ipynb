{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, read about RNNs (Recurrent Neural Networks).  \n",
    "\n",
    "1. Understand it's difference from the FFNNs. (Write your answer down below)  \n",
    "\n",
    "https://towardsdatascience.com/recurrent-neural-networks-rnn-explained-the-eli5-way-3956887e8b75\n",
    "\n",
    "https://towardsdatascience.com/learn-how-recurrent-neural-networks-work-84e975feaaf7\n",
    "\n",
    "2. Why do we need recurrent neural networks? \n",
    "3. For which tasks it would work better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Your answer here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. Your answer here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. Your answer here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60236</td>\n",
       "      <td>b3925e41b823f473</td>\n",
       "      <td>\"\\n\\nThank you Ian. I knew about WP:NOTCENSORE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[``, thank, ian, knew, wp, notcensored, also, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116612</td>\n",
       "      <td>b686d9f97deab4ad</td>\n",
       "      <td>Oh. I never took your comments in any negative...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[oh, never, took, comment, negative, way, perf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72935</td>\n",
       "      <td>d96a1c99002f9cfc</td>\n",
       "      <td>Village pump and newbie \\n\\nI think your handl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[village, pump, newbie, think, handling, newbi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30137</td>\n",
       "      <td>59a0576f85786c1f</td>\n",
       "      <td>I didn't change it hence this BS claim of me s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[n't, change, hence, b, claim, saying, keep, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148580</td>\n",
       "      <td>0f701c200f54455c</td>\n",
       "      <td>What the hell do you people expect? Wikipedia'...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[hell, people, expect, wikipedia, 's, controll...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                id  \\\n",
       "0   60236  b3925e41b823f473   \n",
       "1  116612  b686d9f97deab4ad   \n",
       "2   72935  d96a1c99002f9cfc   \n",
       "3   30137  59a0576f85786c1f   \n",
       "4  148580  0f701c200f54455c   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  \"\\n\\nThank you Ian. I knew about WP:NOTCENSORE...      0             0   \n",
       "1  Oh. I never took your comments in any negative...      0             0   \n",
       "2  Village pump and newbie \\n\\nI think your handl...      0             0   \n",
       "3  I didn't change it hence this BS claim of me s...      0             0   \n",
       "4  What the hell do you people expect? Wikipedia'...      1             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \\\n",
       "0        0       0       0              0   \n",
       "1        0       0       0              0   \n",
       "2        0       0       0              0   \n",
       "3        0       0       0              0   \n",
       "4        1       0       1              1   \n",
       "\n",
       "                                             cleaned  toxicity  \n",
       "0  [``, thank, ian, knew, wp, notcensored, also, ...         0  \n",
       "1  [oh, never, took, comment, negative, way, perf...         0  \n",
       "2  [village, pump, newbie, think, handling, newbi...         0  \n",
       "3  [n't, change, hence, b, claim, saying, keep, a...         0  \n",
       "4  [hell, people, expect, wikipedia, 's, controll...         4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the DF created during the previous task\n",
    "\n",
    "df_binary = pd.read_json(\"../jigsaw-toxic-comment-classification-challenge/df_binary.json\")\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with small amount of this data: \n",
    "df_sample, _ = train_test_split(df_binary, test_size=0.7, stratify=df_binary['obscene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size before filtering: 110213\n",
      "Vocab size after filtering: 42419\n"
     ]
    }
   ],
   "source": [
    "def flat_nested(nested):\n",
    "    flatten = []\n",
    "    for item in nested:\n",
    "        if isinstance(item, list):\n",
    "            flatten.extend(item)\n",
    "        else:\n",
    "            flatten.append(item)\n",
    "    return flatten\n",
    "\n",
    "cnt_vocab = Counter(flat_nested(df_sample.cleaned.tolist()))\n",
    "\n",
    "print(\"Vocab size before filtering: {}\".format(len(cnt_vocab)))\n",
    "\n",
    "threshold_count_l = 1\n",
    "threshold_count_h = 500\n",
    "threshold_len = 2\n",
    "\n",
    "cleaned_vocab = [token for token, count in cnt_vocab.items() if \n",
    "                     threshold_count_h > count > threshold_count_l and len(token) > threshold_len\n",
    "                ]\n",
    "print(\"Vocab size after filtering: {}\".format(len(cleaned_vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_vocab.append(\" \")\n",
    "# Convert list to set \n",
    "cleaned_vocab = set(cleaned_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {v: k for k, v in enumerate(sorted(cleaned_vocab))}\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing our raw text to the model we need to represent each raw text by a vector.   \n",
    "Let's do this by creating an empty list with all of the tokens in it represented by its id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data, token_to_id, max_len=None, dtype='int32', batch_first=True):\n",
    "    \"\"\"\n",
    "    Casts a list of tokens into rnn-digestable matrix\n",
    "        \"data\" contains only sequences represented by tokens from the dictionary, filter noise before \n",
    "    \"\"\"\n",
    "    seq_lengths = list(map(len, data))\n",
    "    max_len = max_len or max(map(len, data))\n",
    "    # Create a marix with a shape [batch size, max number of tokens in sequence]\n",
    "    data_ix = np.zeros([len(data), max_len], dtype) + token_to_id[' ']\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        line_ix = [token_to_id[c] for c in data[i]]\n",
    "        data_ix[i, :len(line_ix)] = line_ix\n",
    "\n",
    "    return data_ix, seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_noise_tokens(df, cleaned_vocab): \n",
    "    df['filtered_tokens'] = df.cleaned.apply(lambda x: [tok for tok in x if tok in cleaned_vocab])\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorhunova/Projects/Python/flair_research/flair_ner/env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# After applying this function there would be sentences with all tokens filtered - empty lists. \n",
    "df_sample = filter_noise_tokens(df_sample, cleaned_vocab)\n",
    "\n",
    "# Remove examples without any tokens assigned \n",
    "df_filtered = df_sample[df_sample.astype(str)['filtered_tokens'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split stratified (would be imbalanced)\n",
    "df_train, df_test = train_test_split(df_filtered, test_size=0.4, stratify=df_filtered['obscene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (26392, 12)\n",
      "Test shape: (17596, 12)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape: {}\".format(df_train.shape))\n",
    "print(\"Test shape: {}\".format(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLoop(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_tokens, emb_size=200, hid_size=128):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, hid_size, batch_first=True)\n",
    "        self.logits = nn.Linear(hid_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, seq_lengths):\n",
    "        # Embed the obtained sequence \n",
    "        emb = self.emb(x)\n",
    "        # Pack padded sequence - why do we need this, refer to:\n",
    "        # https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
    "        \n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(emb,\n",
    "                                                   seq_lengths,\n",
    "                                                   batch_first=True,\n",
    "                                                   enforce_sorted=False\n",
    "                                                  ) \n",
    "        all_hidden_states, hidden = self.rnn(pack)\n",
    "        logits = self.logits(hidden)\n",
    "        # Cast logits to the range from 0 to 1 \n",
    "        output = self.sigmoid(logits)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for 1 epoch: 412\n",
      "Epoch 0: train loss: 0.15799956292691597\n",
      "Epoch 1: train loss: 0.09098772610564845\n",
      "Epoch 2: train loss: 0.06469609696718695\n",
      "Epoch 3: train loss: 0.055170654709726714\n",
      "Epoch 4: train loss: 0.05423462519691173\n",
      "Epoch 5: train loss: 0.05278378341785509\n",
      "Epoch 6: train loss: 0.04587735820254222\n",
      "Epoch 7: train loss: 0.04313932653591123\n",
      "Epoch 8: train loss: 0.036186877298745285\n",
      "Epoch 9: train loss: 0.02939191049397744\n"
     ]
    }
   ],
   "source": [
    "# Initialise the model \n",
    "model = RNNLoop(num_tokens=len(cleaned_vocab))\n",
    "# specify loss function\n",
    "criterion = nn.BCELoss()\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
    "history = []\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "n_iters = df_train.shape[0] // batch_size\n",
    "print(\"Number of iterations for 1 epoch: {}\".format(n_iters))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0 \n",
    "    for step in range(n_iters):\n",
    "\n",
    "        optimizer.zero_grad()    # Forward pass\n",
    "        # Make a random sample from the dataframe \n",
    "        sample = df_train.sample(batch_size)\n",
    "\n",
    "        # Vectorize the obtained sample \n",
    "        batch_ix, seq_lengths = vectorize(sample.filtered_tokens.tolist(), token_to_id)\n",
    "        # Convert vectorized batch to tensor \n",
    "        batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "\n",
    "        # Select true labels \n",
    "        y_true = sample.obscene.tolist()\n",
    "        # Convert true labels to tensor \n",
    "        y_true = torch.tensor(y_true, dtype=torch.float)\n",
    "\n",
    "        # Make prediction \n",
    "        y_pred = model(batch_ix, seq_lengths)\n",
    "\n",
    "        loss = criterion(y_pred.squeeze(), y_true)\n",
    "\n",
    "        epoch_loss += loss.item() / n_iters\n",
    "        loss.backward()   # Backward pass \n",
    "        optimizer.step()\n",
    "            \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, epoch_loss))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for test dataset splitting on batches \n",
    "\n",
    "def index_marks(nrows, chunk_size):\n",
    "    return range(1 * chunk_size, (nrows // chunk_size + 1) * chunk_size, chunk_size)\n",
    "\n",
    "def split(df, chunk_size):\n",
    "    indices = index_marks(df.shape[0], chunk_size)\n",
    "    return np.split(df, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, df_test, batch_size, threshold): \n",
    "    n_prints = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    # Split data in batches \n",
    "    test_batches = split(df_test, batch_size)\n",
    "    \n",
    "    for batch in test_batches:\n",
    "        # Vectorize batches\n",
    "        batch_ix, seq_lengths = vectorize(batch.filtered_tokens.tolist(), token_to_id)\n",
    "        # Convert vectorized batch to tensor \n",
    "        batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "\n",
    "        # Select true labels \n",
    "        y_true = batch.obscene.tolist()\n",
    "\n",
    "        # Make prediction \n",
    "        y_pred = model(batch_ix, seq_lengths).detach().squeeze().numpy()\n",
    "        # Convert it to binaries \n",
    "        y_pred = [int(pred.item() > threshold) for pred in y_pred]\n",
    "        \n",
    "        # Add them to parallel lists \n",
    "        predictions.extend(y_pred)\n",
    "        true_labels.extend(y_true)\n",
    "        \n",
    "        # Print some examples with obscene documents texts and predicted and true labels \n",
    "        for true, pred, document in zip(y_true, y_pred, batch.comment_text):\n",
    "            if true == 1.0 and n_prints < 10:\n",
    "                print(\"Predicted label: {}\".format(pred))\n",
    "                print(\"True label: {}\".format(true))\n",
    "                print(\"Document: {}\".format(document))\n",
    "                print(\"*-*-\"*20)\n",
    "                n_prints += 1\n",
    "        \n",
    "    return true_labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 0\n",
      "True label: 1\n",
      "Document: Do you think people would like you more if you weren't such a dick? Or are you a dick because no one likes you? Either way, you won't be missed.\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 1\n",
      "True label: 1\n",
      "Document: GET A LIFE shit Nerd!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 1\n",
      "True label: 1\n",
      "Document: YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK YOU CAN SUCK MY LICK\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1\n",
      "Document: \"\n",
      "That was a nice message on my talk page. I enjoyed it. I mean, you still suck, but yeah, I had fun.  talk \"\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1\n",
      "Document: William Avery is a Flaming homosexual from gay ass britain\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1\n",
      "Document: \"\n",
      "\n",
      " Here's a Falafel for you you fat piece of shit. \n",
      "\n",
      "  HURRICANEFAN25! Y U NO LET ME VANDALIZE!? PLEASE STOP OR I WILL BE FORCE TO MUSHROOM STAMP YOU WITH A FUCKING GOAT HORN! The Game.   \"\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 1\n",
      "True label: 1\n",
      "Document: Fuck off!! \n",
      "\n",
      "Go and buy some Andrex and wipe your rats ass/face (delete as appropriate). Fuck you!!!\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1\n",
      "Document: What in the fuck......will someone please revert this O.o\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1\n",
      "Document: He's gonna take you back to the past... to play some shitty games that suck ass...\n",
      "\n",
      "Leave me a message...\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Predicted label: 0\n",
      "True label: 1\n",
      "Document: Better phrasing, thank you. But why did you reinsert Cantarella, and what is she doing in a section exclusively about Percy?! And where does Percy say that most Greeks fucked their boys up the ass?\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n"
     ]
    }
   ],
   "source": [
    "true_labels, predictions = make_predictions(model, df_test, batch_size=64, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96     16629\n",
      "           1       0.37      0.49      0.42       967\n",
      "\n",
      "    accuracy                           0.93     17596\n",
      "   macro avg       0.67      0.72      0.69     17596\n",
      "weighted avg       0.94      0.93      0.93     17596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pring a classification report: \n",
    "\n",
    "print(classification_report(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "1. Make a dataset balanced: for example select all of the obscene messages, calculate its number and sample from the clean messages equal number of examples. **(1)See if it increased your score on toxic messages.** \n",
    "\n",
    "As the **additional** task you can modify your dataset sampling during the training/testing. Read about Datasets, DataSamplers and DataLoaders in pytorch. Try to apply them. \n",
    "\n",
    "\n",
    "2. Read about RNNs different types (LSTMs and GRUs): \n",
    "  https://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n",
    "\n",
    "  https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 \n",
    "  \n",
    "  **(2)What is the difference between RNN and LSTM? Why do we need LSTM? Explain it in your own words.**  \n",
    "  \n",
    "  **(3)What is the difference between LSTM and GRU? Explain it in your own words.** \n",
    "  \n",
    "  \n",
    "3. Modify your network to make it possible to work with nn.LSTM or nn.GRU layers. (Their outputs may be a little bit defferent from nn.RNN, so be careful to modify your code accordingly). \n",
    "\n",
    "4. Compare all of the previous examples: classification with RNN (or LSTM/GRU) and FFNN. **(4)Which one performed better according to the metrics? (5)To the time?**\n",
    "\n",
    "5. **(6)How dataset imbalancing are influencing your model? Read about dataset imbalancing and about possibilities to handle them. (7)Write down below what can we do with it, or implement a solution.** \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, answer the questions 1-7 and write your answers down below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answers here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
