{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim==3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models import FastText\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numexpr as ne\n",
    "\n",
    "ne.set_num_threads(ne.detect_number_of_cores())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../jigsaw-toxic-comment-classification-challenge/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(tokenizer, lemmatizer, stop_words, punctuation, text): \n",
    "    tokens = tokenizer(text.lower())\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return [token for token in lemmas if token not in stop_words and token not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_load = True\n",
    "\n",
    "if not bool_load:\n",
    "    df['cleaned'] = df.comment_text.apply(lambda x: preprocess_text(word_tokenize, lemmatizer, stop_words, punctuation, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_save = False\n",
    "\n",
    "if bool_save:\n",
    "    df.to_csv(\"../jigsaw-toxic-comment-classification-challenge/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139960</th>\n",
       "      <td>139960</td>\n",
       "      <td>ed0b00459b116c89</td>\n",
       "      <td>\"\\nMaybe before you start making changes of yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['``', 'maybe', 'start', 'making', 'change', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112026</th>\n",
       "      <td>112026</td>\n",
       "      <td>5766d8a3f2b5bb97</td>\n",
       "      <td>:Rob rachlin\\nPlease do not make personal atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['rob', 'rachlin', 'please', 'make', 'personal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59726</th>\n",
       "      <td>59726</td>\n",
       "      <td>9feb4d19f9e237fe</td>\n",
       "      <td>Help desk is closed for me. Who made this?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['help', 'desk', 'closed', 'made']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83678</th>\n",
       "      <td>83678</td>\n",
       "      <td>dff003d90c36b7e6</td>\n",
       "      <td>\"\\nAnd your sources in Serbian which you use t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['``', 'source', 'serbian', 'use', 'article', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130464</th>\n",
       "      <td>130464</td>\n",
       "      <td>b9eb04bfc01ac175</td>\n",
       "      <td>A victory means the enemy was destroyed, in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['victory', 'mean', 'enemy', 'wa', 'destroyed'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                id  \\\n",
       "139960      139960  ed0b00459b116c89   \n",
       "112026      112026  5766d8a3f2b5bb97   \n",
       "59726        59726  9feb4d19f9e237fe   \n",
       "83678        83678  dff003d90c36b7e6   \n",
       "130464      130464  b9eb04bfc01ac175   \n",
       "\n",
       "                                             comment_text  toxic  \\\n",
       "139960  \"\\nMaybe before you start making changes of yo...      0   \n",
       "112026  :Rob rachlin\\nPlease do not make personal atta...      0   \n",
       "59726          Help desk is closed for me. Who made this?      0   \n",
       "83678   \"\\nAnd your sources in Serbian which you use t...      0   \n",
       "130464  A victory means the enemy was destroyed, in th...      0   \n",
       "\n",
       "        severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "139960             0        0       0       0              0   \n",
       "112026             0        0       0       0              0   \n",
       "59726              0        0       0       0              0   \n",
       "83678              0        0       0       0              0   \n",
       "130464             0        0       0       0              0   \n",
       "\n",
       "                                                  cleaned  \n",
       "139960  ['``', 'maybe', 'start', 'making', 'change', '...  \n",
       "112026  ['rob', 'rachlin', 'please', 'make', 'personal...  \n",
       "59726                  ['help', 'desk', 'closed', 'made']  \n",
       "83678   ['``', 'source', 'serbian', 'use', 'article', ...  \n",
       "130464  ['victory', 'mean', 'enemy', 'wa', 'destroyed'...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train our first model based on the vocabulary from df_sample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With initialization model trained for 5 epochs \n",
    "\n",
    "df_sample_cleaned_list = [literal_eval(s) for s in df_sample.cleaned.tolist()]\n",
    "\n",
    "model = Word2Vec(sentences=df_sample_cleaned_list, \n",
    "         size=100,      # embedding vector size\n",
    "         min_count=5,   # consider words that occured at least 5 times\n",
    "         window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99866609, 118158270)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Continue training the model \n",
    "\n",
    "model.train(sentences=df_sample_cleaned_list, \n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=30\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.vocab # to look at vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('others', 0.6589839458465576),\n",
       " ('thing', 0.5865795016288757),\n",
       " ('person', 0.5735608339309692),\n",
       " ('editor', 0.5502363443374634),\n",
       " ('admins', 0.5481014847755432),\n",
       " ('everyone', 0.5176295042037964),\n",
       " ('really', 0.515514612197876),\n",
       " ('someone', 0.5116820931434631),\n",
       " ('guy', 0.5001378655433655),\n",
       " ('way', 0.4992937445640564)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next approach is to try to use the already pretrained model, which can be downloaded from here:\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "model:   \n",
    "GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\n",
    "    os.getcwd() + os.sep + \"GoogleNews-vectors-negative300.bin\", binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try to use GloVe model too and experiment with it: <- later\n",
    "# import gensim.downloader as api\n",
    "# model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Cosine similarity\n",
    "\n",
    "To measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u . v} {||u||_2 ||v||_2} = cos(\\theta) \\tag{1}$$\n",
    "\n",
    "where $u.v$ is the dot product (or inner product) of two vectors, $||u||_2$ is the norm (or length) of the vector $u$, and $\\theta$ is the angle between $u$ and $v$. This similarity depends on the angle between $u$ and $v$. If $u$ and $v$ are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. \n",
    "\n",
    "<img src=\"cosine_sim.png\" style=\"width:800px;height:250px;\">\n",
    "<caption><center> **Figure 1**: The cosine of the angle between two vectors is a measure of how similar they are</center></caption>\n",
    "\n",
    "**Exercise**: Implement the function `cosine_similarity()` to evaluate similarity between word vectors.\n",
    "\n",
    "**Reminder**: The norm of $u$ is defined as $ ||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(w1, w2):\n",
    "    \"\"\"\n",
    "    Cosine similarity between w1 and w2\n",
    "    \n",
    "    Arguments:\n",
    "        w1 : word vector        \n",
    "        w2 : word vector \n",
    "    Returns:\n",
    "        cosine_similarity \n",
    "    \"\"\"\n",
    "    if (not np.any(w1) or not np.any(w2)): # check input is not zero-vector\n",
    "        return 0\n",
    "    \n",
    "    # Dot product between w1 and w2\n",
    "    dot = np.dot(w1, w2)\n",
    "    # L2 norm of w1\n",
    "    norm_u = np.linalg.norm(w1) \n",
    "    # L2 norm of w2 \n",
    "    norm_v = np.linalg.norm(w2) \n",
    "    # Cosine similarity \n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "father = model.get_vector(\"father\")\n",
    "mother = model.get_vector(\"mother\")\n",
    "\n",
    "ball = model.get_vector(\"ball\")\n",
    "crocodile = model.get_vector(\"crocodile\")\n",
    "\n",
    "france = model.get_vector(\"france\")\n",
    "paris = model.get_vector(\"paris\")\n",
    "italy = model.get_vector(\"italy\")\n",
    "rome = model.get_vector(\"rome\")\n",
    "\n",
    "kiev = model.get_vector(\"kiev\")\n",
    "ukraine = model.get_vector(\"ukraine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.79014826\n",
      "cosine_similarity(ball, crocodile) =  0.10283584\n",
      "cosine_similarity(france - paris, rome - italy) =  -0.1988747\n",
      "cosine_similarity(kiev, ukraine) =  0.3738725\n"
     ]
    }
   ],
   "source": [
    "fast_print = lambda u, v, tag1, tag2: print(\n",
    "    \"cosine_similarity({t1}, {t2}) = \".format(t1 = tag1, t2 = tag2), cosine_similarity(u, v)\n",
    ")\n",
    "\n",
    "fast_print(father, mother, \"father\", \"mother\")\n",
    "fast_print(ball, crocodile, \"ball\", \"crocodile\")\n",
    "fast_print(france - paris, rome - italy, \"france - paris\", \"rome - italy\")\n",
    "fast_print(kiev, ukraine, \"kiev\", \"ukraine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approximate expected output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(father, mother)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.79014826\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(ball, crocodile)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         0.10283585\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **cosine_similarity(france - paris, rome - italy)** =\n",
    "        </td>\n",
    "        <td>\n",
    "         -0.421037\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Word analogy task\n",
    "\n",
    "In the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. In detail, we are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n",
    "\n",
    "**Exercise**: Complete the code below to be able to perform word analogies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: here you will need to complete a function in the sections, which are marked as:\n",
    "\n",
    "```\n",
    "# ----- Start ----- #\n",
    "Your code should be written in-between the lines\n",
    "# ------ End ------ #\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_analogy(word_1, word_2, word_3, model):\n",
    "    \"\"\"\n",
    "    Finds the word to complete analogy (see explanation above): a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_1 -- a word, string\n",
    "    word_2 -- a word, string\n",
    "    word_3 -- a word, string\n",
    "    model -- word embeddings model \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_1 - v_2 is close to v_best_word - v_3, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    # convert words to lower case\n",
    "    word_1, word_2, word_3 = word_1.lower(), word_2.lower(), word_3.lower()\n",
    "    \n",
    "    # ----- Start ----- #\n",
    "    # Get the word embeddings v_a, v_b and v_c (â‰ˆ1-3 lines)\n",
    "    fast_get = lambda word: model.get_vector(word)\n",
    "    e_1, e_2, e_3 = tuple(map(fast_get, [word_1, word_2, word_3]))\n",
    "    # ------ End ------ #\n",
    "    \n",
    "    words = list(model.vocab.keys())\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None\n",
    "\n",
    "    # Loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        e_j = fast_get(w)\n",
    "        # to avoid best_word being one of the input words, skip them and continue iteration.\n",
    "        if w in [word_1, word_2, word_3]:\n",
    "            continue\n",
    "        \n",
    "        # ----- Start ----- #\n",
    "        # Compute cosine similarity between the vector (e_2 - e_1) and the vector ((w's vector) - e_3)\n",
    "        cosine_sim = cosine_similarity(e_2 - e_1, e_j - e_3)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "        # do not forget to set new max_cosine_sim to the current value and best_word as well\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        # ------ End ------ #\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man -> woman :: king -> queen\n",
      "bad -> good :: sad -> wonderful\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [\n",
    "    ('man', 'woman', 'king'), \n",
    "    ('bad', 'good', 'sad'), \n",
    "    ('man', 'woman', 'boy'), \n",
    "    ('small', 'smaller', 'large')\n",
    "]\n",
    "\n",
    "for triad in triads_to_try:\n",
    "    print('{} -> {} :: {} -> {}'.format(*triad, find_word_analogy(*triad, model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **man -> woman** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         king -> queen\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **bad -> good** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         sad -> wonderful\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **man -> woman ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         boy -> girl\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **small -> smaller ** ::\n",
    "        </td>\n",
    "        <td>\n",
    "         large -> larger\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next part of the task is to:  \n",
    "\n",
    "1. Train your own W2V model using the proposed method above. Use all of the tokens created after your preprocessing pipeline in the previous tasks. (deleting stop_words, punctuation, lowercasing, etc - play as you want).  \n",
    "2. Use obtained vectors to obtain text vectors using such pipeline: \n",
    "  1. For each word in a preprocessed text, get a word vector from the W2V model. \n",
    "  2. Add them together to obtain vectors for texts (sum them together, or get mean vector) \n",
    "3. Use obtained text vectors as a text representation to perform a text classification task.  \n",
    "   Proposed - use binary classification (for example: select only 'obscene' text and clean and try to distinguish them one from another)\n",
    "4. Calculate the metrics - TP, FP, FN, TN, precision, recall, F1 score, F2 score, accurary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "class callback_custom(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "         self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Iteration {:3}\".format(self.epoch+1))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   1\n",
      "Iteration   2\n",
      "Iteration   3\n",
      "Iteration   4\n",
      "Iteration   5\n"
     ]
    }
   ],
   "source": [
    "# init w2v model\n",
    "n_dimensions = 300\n",
    "\n",
    "model_w2v = Word2Vec(sentences=df_sample_cleaned_list, \n",
    "                     size=n_dimensions, min_count=5, window=5,\n",
    "                     callbacks=[callback_custom()]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   6\n",
      "Iteration   7\n",
      "Iteration   8\n",
      "Iteration   9\n",
      "Iteration  10\n",
      "Iteration  11\n",
      "Iteration  12\n",
      "Iteration  13\n",
      "Iteration  14\n",
      "Iteration  15\n",
      "Iteration  16\n",
      "Iteration  17\n",
      "Iteration  18\n",
      "Iteration  19\n",
      "Iteration  20\n",
      "Iteration  21\n",
      "Iteration  22\n",
      "Iteration  23\n",
      "Iteration  24\n",
      "Iteration  25\n",
      "Iteration  26\n",
      "Iteration  27\n",
      "Iteration  28\n",
      "Iteration  29\n",
      "Iteration  30\n",
      "Iteration  31\n",
      "Iteration  32\n",
      "Iteration  33\n",
      "Iteration  34\n",
      "Iteration  35\n",
      "Iteration  36\n",
      "Iteration  37\n",
      "Iteration  38\n",
      "Iteration  39\n",
      "Iteration  40\n",
      "Iteration  41\n",
      "Iteration  42\n",
      "Iteration  43\n",
      "Iteration  44\n",
      "Iteration  45\n",
      "Iteration  46\n",
      "Iteration  47\n",
      "Iteration  48\n",
      "Iteration  49\n",
      "Iteration  50\n",
      "Iteration  51\n",
      "Iteration  52\n",
      "Iteration  53\n",
      "Iteration  54\n",
      "Iteration  55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(166440075, 196930450)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "number_of_iterations = 50\n",
    "\n",
    "model_w2v.train(sentences=df_sample_cleaned_list, \n",
    "            total_examples=model_w2v.corpus_count,\n",
    "            epochs=number_of_iterations\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('others', 0.4841853976249695),\n",
       " ('thing', 0.4806921184062958),\n",
       " (\"n't\", 0.46461012959480286),\n",
       " ('way', 0.46231457591056824),\n",
       " ('editor', 0.4622182548046112),\n",
       " ('person', 0.45410239696502686),\n",
       " ('admins', 0.4445332884788513),\n",
       " ('really', 0.4407009482383728),\n",
       " ('would', 0.4344203472137451),\n",
       " (\"'re\", 0.4250907301902771)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('two', 0.5056474208831787),\n",
       " (\"n't\", 0.47986412048339844),\n",
       " ('way', 0.47343599796295166),\n",
       " (\"'s\", 0.4682750105857849),\n",
       " ('article', 0.4649146795272827),\n",
       " ('even', 0.45324623584747314),\n",
       " ('many', 0.4527236521244049),\n",
       " ('also', 0.4462509751319885),\n",
       " ('thing', 0.4392848610877991),\n",
       " ('think', 0.43367040157318115)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_save_model = False\n",
    "\n",
    "if bool_save_model:\n",
    "    model_w2v.wv.save_word2vec_format('w2v_df_t2_clnd_sample.bin', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_vectors = model_w2v.wv # getting keyed vectors from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building text vectors\n",
    "def form_text_vector(words_from_text, w2v_model_keyed_vectors, num_dim, tfidf_matr = None, idx_doc = None, vocab = None):\n",
    "    text_vectorized = np.zeros(num_dim)\n",
    "    for k in range(len(words_from_text)):\n",
    "        try:\n",
    "            q = tfidf_matr[\n",
    "                idx_doc, vocab.index(words_from_text[k])\n",
    "            ] if tfidf_matr is not None else 1\n",
    "            v = w2v_model_keyed_vectors[words_from_text[k]]\n",
    "            #v = w2v_model_keyed_vectors.get_vector(words_from_text[k])\n",
    "            text_vectorized = ne.evaluate('text_vectorized + q * v') \n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    return text_vectorized\n",
    "\n",
    "\n",
    "def form_corpus_matrix(corpus, w2v_model_keyed_vectors, num_dim, weightened = False):\n",
    "    '''\n",
    "    corpus -> list of lists of strings\n",
    "    w2v_model_keyed_vectors -> word2vec.wv\n",
    "    num_dim -> dimension of word2vec vectors\n",
    "    weightened -> use tf-idf weighting on vector components or not\n",
    "    '''\n",
    "    fast_vocab = lambda word_model: list(word_model.wv.vocab.keys())\n",
    "    fast_concat = lambda text_data: [' '.join(e) for e in text_data]\n",
    "    corpus_len = len(corpus)\n",
    "    corpus_vectorized = np.empty((corpus_len, num_dim))\n",
    "    tfidf = None if not weightened else TfidfVectorizer(\n",
    "        vocabulary = fast_vocab(w2v_model_keyed_vectors)\n",
    "    ).fit_transform(fast_concat(corpus))\n",
    "    for j in range(corpus_len):\n",
    "        corpus_vectorized[j] = form_text_vector(\n",
    "            corpus[j], w2v_model_keyed_vectors, num_dim, tfidf, j, fast_vocab(w2v_model_keyed_vectors)\n",
    "        )\n",
    "    return corpus_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                              texts\n",
      "0      1           ['cocksucker', 'piss', 'around', 'work']\n",
      "1      1  ['gay', 'antisemmitian', 'archangel', 'white',...\n",
      "2      1                ['fuck', 'filthy', 'mother', 'dry']\n",
      "3      1  ['stupid', 'peace', 'shit', 'stop', 'deleting'...\n",
      "4      1  ['=tony', 'sidaway', 'obviously', 'fistfuckee'...\n",
      "\n",
      "        label                                              texts\n",
      "151218      0  ['``', 'second', 'time', 'asking', 'view', 'co...\n",
      "151219      0  ['ashamed', 'horrible', 'thing', 'put', 'talk'...\n",
      "151220      0  ['spitzer', 'umm', 'actual', 'article', 'prost...\n",
      "151221      0  ['look', 'like', 'wa', 'actually', 'put', 'spe...\n",
      "151222      0  ['``', '...', 'really', \"n't\", 'think', 'under...\n"
     ]
    }
   ],
   "source": [
    "text_categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate', 'cleaned']\n",
    "\n",
    "temp = df[[text_categories[4], text_categories[-1]]]\n",
    "\n",
    "temp_n = temp[~df[text_categories[:-1]].any(axis = 'columns')]\n",
    "temp_i = temp[df.insult != 0]\n",
    "\n",
    "insulting_and_neutral = temp_i.append(temp_n).reset_index(drop = True)\n",
    "insulting_and_neutral.columns = ['label', 'texts']\n",
    "\n",
    "del temp, temp_n, temp_i\n",
    "\n",
    "print(\n",
    "    insulting_and_neutral.head(),\n",
    "    insulting_and_neutral.tail(),\n",
    "    sep = '\\n\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 0.25\n",
    "\n",
    "X_train_t, X_test_t, Y_train, Y_test = train_test_split(\n",
    "    insulting_and_neutral['texts'], insulting_and_neutral['label'],\n",
    "    test_size = P,\n",
    "    random_state = 1\n",
    ")\n",
    "\n",
    "X_train_t = [literal_eval(s) for s in X_train_t.reset_index(drop = True)]\n",
    "X_test_t = [literal_eval(s) for s in X_test_t.reset_index(drop = True)]\n",
    "\n",
    "Y_train = Y_train.reset_index(drop = True)\n",
    "Y_test = Y_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_t[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_t[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_report(y_test, y_prediction):\n",
    "    confusion_matr = confusion_matrix(y_test, y_prediction)\n",
    "    print(\"CONFUSION MATRIX:\\n{matr}\".format(matr=confusion_matr))\n",
    "    accuracy_of_model = accuracy_score(y_test, y_prediction)\n",
    "    print(\"ACCURACY:\\n{acc}\".format(acc = accuracy_of_model))\n",
    "    sklearn_report = classification_report(y_test, y_prediction)\n",
    "    print(\"TABLE:\\n{tab}\".format(tab=sklearn_report))\n",
    "    return (confusion_matr, accuracy_of_model, sklearn_report)\n",
    "\n",
    "# too slow, rewrite later (1)\n",
    "def quick_init_and_train_word_cls_model_no_args(\n",
    "    x_train_t, x_test_t, y_train, y_test, word_model, n_dim, cls_model, weightened = False):\n",
    "    print('DATA PREPAIRING START')\n",
    "    x_train = form_corpus_matrix(\n",
    "        x_train_t, w2v_model_keyed_vectors = word_model, num_dim = n_dim, weightened = weightened\n",
    "    ) # form_corpus_matrix(x_train_t, word_model, n_dim, weightened)\n",
    "    x_test = form_corpus_matrix(\n",
    "        x_test_t, w2v_model_keyed_vectors = word_model, num_dim = n_dim, weightened = weightened\n",
    "    )# form_corpus_matrix(x_test_t, word_model, n_dim, weightened)\n",
    "    print('MODEL TRAINING START')\n",
    "    cls_m = cls_model()\n",
    "    cls_m.fit(x_train, y_train)\n",
    "    \n",
    "    y_prediction = cls_m.predict(x_test)\n",
    "    results = basic_report(y_test, y_prediction)\n",
    "    \n",
    "    return (cls_m, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPAIRING START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TRAINING START\n",
      "CONFUSION MATRIX:\n",
      "[[35772    43]\n",
      " [ 1046   945]]\n",
      "ACCURACY:\n",
      "0.971195048405015\n",
      "TABLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99     35815\n",
      "           1       0.96      0.47      0.63      1991\n",
      "\n",
      "    accuracy                           0.97     37806\n",
      "   macro avg       0.96      0.74      0.81     37806\n",
      "weighted avg       0.97      0.97      0.97     37806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# include classifier: RF\n",
    "random_forest_cls_dat_1 = quick_init_and_train_word_cls_model_no_args(\n",
    "    X_train_t, X_test_t, Y_train, Y_test, model_w2v_vectors, n_dimensions, RandomForestClassifier, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPAIRING START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TRAINING START\n",
      "CONFUSION MATRIX:\n",
      "[[35645   170]\n",
      " [  631  1360]]\n",
      "ACCURACY:\n",
      "0.9788128868433582\n",
      "TABLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     35815\n",
      "           1       0.89      0.68      0.77      1991\n",
      "\n",
      "    accuracy                           0.98     37806\n",
      "   macro avg       0.94      0.84      0.88     37806\n",
      "weighted avg       0.98      0.98      0.98     37806\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# include classifier: LR\n",
    "logit_cls_dat_1 = quick_init_and_train_word_cls_model_no_args(\n",
    "    X_train_t, X_test_t, Y_train, Y_test, model_w2v_vectors, n_dimensions, LogisticRegression, False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The second part of the task is: \n",
    "\n",
    "1. While performing a step 2 for text vectorization, for each word add its vector with tf-idf weight -> weighted average. \n",
    "2. Perform a same text classification task as it was required above. \n",
    "3. Calculate the metrics, compare with a vectorization approach without weightning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPAIRING START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TRAINING START\n",
      "CONFUSION MATRIX:\n",
      "[[35777    38]\n",
      " [ 1041   950]]\n",
      "ACCURACY:\n",
      "0.9714595566841242\n",
      "TABLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99     35815\n",
      "           1       0.96      0.48      0.64      1991\n",
      "\n",
      "    accuracy                           0.97     37806\n",
      "   macro avg       0.97      0.74      0.81     37806\n",
      "weighted avg       0.97      0.97      0.97     37806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# include classifier: RF\n",
    "random_forest_cls_dat_2 = quick_init_and_train_word_cls_model_no_args(\n",
    "    X_train_t, X_test_t, Y_train, Y_test, model_w2v_vectors, n_dimensions, RandomForestClassifier, True\n",
    ") # (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The third part of the task is: \n",
    "\n",
    "1. Use a pre-trained W2V model for obtaining a word vectors for each of the tokens in your dataset, create text vectors WITHOUT weightning. \n",
    "2. Train text classification model.\n",
    "3. Calculate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPAIRING START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:35: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-47c920b214a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# include classifier: RF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m random_forest_cls_dat_3 = quick_init_and_train_word_cls_model_no_args_w2v(\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mX_train_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_dimensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-49-a7488b45dd13>\u001b[0m in \u001b[0;36mquick_init_and_train_word_cls_model_no_args_w2v\u001b[1;34m(x_train_t, x_test_t, y_train, y_test, word_model, n_dim, cls_model, weightened)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DATA PREPAIRING START'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     x_train = form_corpus_matrix(\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mx_train_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_model_keyed_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweightened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweightened\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     ) # form_corpus_matrix(x_train_t, word_model, n_dim, weightened)\n\u001b[0;32m     17\u001b[0m     x_test = form_corpus_matrix(\n",
      "\u001b[1;32m<ipython-input-45-555a2787e0e2>\u001b[0m in \u001b[0;36mform_corpus_matrix\u001b[1;34m(corpus, w2v_model_keyed_vectors, num_dim, weightened)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         corpus_vectorized[j] = form_text_vector(\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_model_keyed_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfast_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_model_keyed_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorpus_vectorized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# include classifier: RF\n",
    "random_forest_cls_dat_3 = quick_init_and_train_word_cls_model_no_args(\n",
    "    X_train_t, X_test_t, Y_train, Y_test, model, n_dimensions, RandomForestClassifier\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fourth part of the task is: \n",
    "\n",
    "1. Use a pre-trained W2V model for obtaining a word vectors for each of the tokens in your dataset, create text vectors WITH tf-idf weightning. \n",
    "2. Train a text classification model. \n",
    "3. Calculate the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include classifier: RF\n",
    "random_forest_cls_dat_4 = quick_init_and_train_word_cls_model_no_args(\n",
    "    X_train_t, X_test_t, Y_train, Y_test, model, n_dimensions, RandomForestClassifier, True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dimentionality reduction methods such as t-SNE or PCA to make your 300 dim vectors available for 2D plotting. \n",
    "\n",
    "Select top (10-20) words for each cathegory BY TF-IDF SCORE, not counts!!! \n",
    "\n",
    "Plot on the ONE plot all of this words but colors must be different for top-words for obscene cathegory, clean, toxic, etc... \n",
    "\n",
    "See, if words from one cathegory are closer to each other than to others. \n",
    "Or you observe ~2 clusters: all of the toxic words, clean words.  \n",
    "Explain what you see and why. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as always, using PCA\n",
    "model_pca = PCA(n_components = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional part: \n",
    "\n",
    "1. Find a pre-trained FastText vectors, understand it's difference from W2V vectors. \n",
    "2. Vectorize all of your texts using FT model, perform a text classification, calculate the metrics, compare with W2V approach. \n",
    "\n",
    "Or/And you can:\n",
    "\n",
    "1. Train your own FT model and make the same. \n",
    "2. Compare it with previous approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastText(sentences=df_sample_cleaned_list, \n",
    "                    size=300,\n",
    "                    min_count=5,\n",
    "                    window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "number_of_iterations = 50\n",
    "\n",
    "model_ft.train(sentences=df_sample_cleaned_list, \n",
    "            total_examples=model_ft.corpus_count,\n",
    "            epochs=number_of_iterations\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPAIRING START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TRAINING START\n",
      "CONFUSION MATRIX:\n",
      "[[35764    51]\n",
      " [ 1112   879]]\n",
      "ACCURACY:\n",
      "0.9692376871396075\n",
      "TABLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     35815\n",
      "           1       0.95      0.44      0.60      1991\n",
      "\n",
      "    accuracy                           0.97     37806\n",
      "   macro avg       0.96      0.72      0.79     37806\n",
      "weighted avg       0.97      0.97      0.96     37806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_cls_dat_5 = quick_init_and_train_word_cls_model_no_args(\n",
    "    X_train_t, X_test_t, Y_train, Y_test, model_ft, n_dimensions, RandomForestClassifier, False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREPAIRING START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\laplace-transform\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TRAINING START\n",
      "CONFUSION MATRIX:\n",
      "[[35769    46]\n",
      " [ 1060   931]]\n",
      "ACCURACY:\n",
      "0.9707453843305296\n",
      "TABLE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98     35815\n",
      "           1       0.95      0.47      0.63      1991\n",
      "\n",
      "    accuracy                           0.97     37806\n",
      "   macro avg       0.96      0.73      0.81     37806\n",
      "weighted avg       0.97      0.97      0.97     37806\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_forest_cls_dat_6 = quick_init_and_train_word_cls_model_no_args(\n",
    "    X_train_t, X_test_t, Y_train, Y_test, model_ft, n_dimensions, RandomForestClassifier, True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving models into c binary files, measures into dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions: \n",
    "\n",
    "Please, provide a clear table or dataframe with all of the metrics for all of the trained/used models available.   \n",
    "\n",
    "Compare them to each other.   \n",
    "\n",
    "Make conclusions which one from your models worked better for this particular task.   \n",
    "BE CAREFUL: Having a better model performance on this particular task does not matter that this model is better than others in GENERAL. You need to make your own conclusions about this particular model applied to this particular task. Please, think and understand WHY.   \n",
    "Write your thoughts down below: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your conclusions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your thoughts about the last question here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
